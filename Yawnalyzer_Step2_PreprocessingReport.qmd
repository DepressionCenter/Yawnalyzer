---
title: "Yawnalyzer"
subtitle: "Preprocessing Report"
author:
  - name: NAME
    email: EMAIL
    affiliations: AFFILIATION
date: last-modified
date-format: "[Last Modified:] DD MMM YYYY"
format:
  html:
    toc: true
    toc-depth: 2
    code-fold: true
    df-print: paged
    embed-resources: true
    html-math-method: mathjax
jupyter: python3
---

```{python}

#| tags: [parameters]
#| echo: false
#| message:
#| code-fold: false

RID = "PARTICIPANT_ID"

```

```{python}

#| label: "Import_Packages" 
#| echo: false
#| message:
#| code-fold: false

import os
import pandas as pd
import numpy as np
#import neurokit2 as nk
import plotly.express as px
import plotly.graph_objects as go
import datetime as dt
from datetime import datetime, timedelta
from IPython.display import Markdown, display
import PathKeeper

```

# About this Report

```{python}

#| code-hide: true
#| echo: false
#| label: ParticipantID


Markdown(f"> Participant: _**{RID}**_")

```

> Generation Date: `{python} dt.datetime.now().strftime("%Y-%m-%d")`

This is a processing report for Subject ID (SID) **`{python} RID`** from the MS-EMA project (IRB #HUM00265263; PI: Tiffany Braley, MD). MS-EMA is an Apple watch-based sleep tracking study conducted with patients with Multiple Sclerosis. 

This report provides both a sense of quality and summary metrics for the following types of information:

- Heart rate (derived from PPG)
- Sleep-related metrics
- Gait and balance

Data for the above were collected over the span of approximately 12 weeks for each subject. Plots contained within this report are summarized by week.


```{python}

#| label: "Functions"
#| echo: false

def prettify_info(df, memory_usage=False, deep=True, sort_by_nulls=False, show_unique=True, show_stats=True):
  n=len(df)
  info_df=pd.DataFrame({
    "Column":df.columns,
    "Total Count" : df.count(),
    "Not-Nulls Count" : df.notnull().sum(),
    "Missing Count" : df.isnull().sum(),
    "Type" : df.dtypes.astype(str)
    })
  if show_unique:
    info_df["Unique Count"] = df.nunique(dropna=True)
  if show_stats:
    numeric_cols = df.select_dtypes(include=[np.number])
    means = numeric_cols.mean()#.round(5)
    medians = numeric_cols.median()#.round(5)
    stds = numeric_cols.std()#.round(5)
    Minimum= numeric_cols.min()
    FirstQuarter= numeric_cols.quantile(0.25)
    ThirdQuarter= numeric_cols.quantile(0.75)
    Maximum = numeric_cols.max()


    info_df["Means"] =  info_df["Column"].map(means).infer_objects().fillna("")
    info_df["Median"] =  info_df["Column"].map(medians).infer_objects().fillna("")
    info_df["Standard Deviation"] =  info_df["Column"].map(stds).infer_objects().fillna("")
    info_df["Min"] =  info_df["Column"].map(Minimum).fillna("")
    info_df["25%"] =  info_df["Column"].map(FirstQuarter).infer_objects().fillna("")
    info_df["75%"] =  info_df["Column"].map(ThirdQuarter).infer_objects().fillna("")
    info_df["Max"] =  info_df["Column"].map(Maximum).infer_objects().fillna("")
  
  base_cols = ["Column", "Total Count","Not-Nulls Count","Missing Count"]
  uniqueness_cols = ["Unique Count"] if show_unique else []
  stat_cols = ["Means", "Median", "Standard Deviation", "Min", "25%", "75%", "Max"] if show_stats else []
  info_df = info_df[base_cols + uniqueness_cols + stat_cols + ["Type"]]

  if memory_usage:
    mem = df.memory_usage(deep=deep).sum / 1024**2
    print(f"Memory usage: {mem.df} MB")

  
  return info_df

```



```{python}

#| label: "Inital-Read"
#| echo: false
#| warning: false

presence_df = pd.read_csv(os.path.join(PathKeeper.data_log_path,"Data_Presence_Log.csv"))

hr_df = pd.read_csv(os.path.join(PathKeeper.merged_data_path,"hr_collapsed.csv"), index_col=["index_1"], dtype={
  "ID": "string",
  "submission_date": "string",
  "file_type": "string",
  "HR": "float64",
  "Timestamp":  "float64",
  "filename": "string" 
 }
)

sleep_df = pd.read_csv(os.path.join(PathKeeper.merged_data_path,"sleep_collapsed.csv"), index_col=["index_1"])

gait_df = pd.read_csv(os.path.join(PathKeeper.merged_data_path,"gait_collapsed.csv"), index_col=["index_1"])

double_support_df = pd.read_csv(os.path.join(PathKeeper.merged_data_path,"double_support_collapsed.csv"), index_col=["index_1"])

asymmetry_df =pd.read_csv(os.path.join(PathKeeper.merged_data_path,"asymmetry_collapsed.csv"), index_col=["index_1"])


hr_df = hr_df[hr_df["ID"] == RID]
sleep_df = sleep_df[sleep_df["ID"] == RID]
gait_df = gait_df[gait_df["ID"] == RID]
double_support_df = double_support_df[double_support_df["ID"] == RID]
asymmetry_df = asymmetry_df[asymmetry_df["ID"] == RID]

has_hr ="hr_df" in locals() and len(hr_df) > 0

has_sleep ="sleep_df" in locals() and len(sleep_df) > 0

has_gait ="gait_df" in locals() and len(gait_df) > 0

has_double_support ="double_support_df" in locals() and len(double_support_df) > 0

has_asymmetry= "asymmetry_df" in locals() and len(asymmetry_df) > 0

```

```{python}

#| label: "Time-Conversions"
#| eval: True

#Sleep 

sleep_df["StartISO"] = pd.to_datetime(sleep_df["StartISO"], utc=True).dt.tz_convert(
    "America/Detroit"
)
sleep_df["EndISO"] = pd.to_datetime(sleep_df["EndISO"], utc=True).dt.tz_convert(
    "America/Detroit"
)

sleep_df["StartISO"] = (
    sleep_df["StartISO"]
    .infer_objects()
    .fillna(pd.to_datetime(sleep_df["Start"], unit="s", utc=True))
)

sleep_df["EndISO"] = (
    sleep_df["EndISO"]
    .infer_objects()
    .fillna(pd.to_datetime(sleep_df["End"], unit="s", utc=True))
)

sleep_df["Duration"] = sleep_df["EndISO"] - sleep_df["StartISO"]

sleep_df["hour"] = sleep_df["EndISO"].dt.hour

sleep_df["Before_or_After_noon"] = np.where(sleep_df["EndISO"].dt.hour < 12,"Morning","Afternoon")

sleep_df["Night_of_Sleep"] = np.where(
    sleep_df["Before_or_After_noon"] == "Morning",
    (sleep_df["EndISO"] - pd.Timedelta(days=1)).dt.date,
    sleep_df["EndISO"].dt.date,
)

sleep_df["Duration"] = sleep_df["Duration"].where(
    sleep_df["Duration"] <= pd.Timedelta(days=1),
)

sleep_df["Night_of_Sleep"] = sleep_df["Night_of_Sleep"].where(
    sleep_df["Duration"].notna()
)

# sleep_df["Night_of_Sleep"] = pd.to_datetime(
#     sleep_df["Night_of_Sleep"], utc=True
# ).dt.tz_convert("America/Detroit")

#sleep_df["Night_of_Sleep"] = sleep_df["Night_of_Sleep"].dt.date

sleep_df["EndISO"] = (
    sleep_df["EndISO"]
    .infer_objects()
    .fillna(pd.to_datetime(sleep_df["End"], unit="s", utc=True))
)

sleep_df=sleep_df.drop(["hour","Before_or_After_noon"], axis=1)


#Heart Rate

hr_df["TimestampISO"] = pd.to_datetime(hr_df["TimestampISO"],errors="coerce", utc=True)


hr_df["TimestampISO"] = (
    hr_df["TimestampISO"]
    .infer_objects()
    .fillna(pd.to_datetime(hr_df["Timestamp"], unit="s", utc=True))
)

hr_df["TimestampISO"] = (
    pd.to_datetime(hr_df["TimestampISO"], utc=True, errors="coerce")
    .dt.tz_convert("America/Detroit")
    .infer_objects()
)   

# Gait
gait_df["TimestampISO"] = (
    gait_df["TimestampISO"]
    .infer_objects()
    .fillna(pd.to_datetime(gait_df["Timestamp"], unit="s", utc=False))
)

# Double Support
double_support_df["TimestampISO"] = (
    double_support_df["TimestampISO"]
    .infer_objects()
    .fillna(pd.to_datetime(double_support_df["Timestamp"], unit="s", utc=False))
)

# Asymmetry

asymmetry_df["TimestampISO"] = (
    asymmetry_df["TimestampISO"]
    .infer_objects()
    .fillna(pd.to_datetime(asymmetry_df["Timestamp"], unit="s", utc=False))
)




```

```{python}
#|label: Sort dataframes by Time

hr_df=hr_df.sort_values(by="TimestampISO")
sleep_df = sleep_df.sort_values(by="StartISO")
gait_df = gait_df.sort_values(by="TimestampISO")
double_support_df = double_support_df.sort_values("TimestampISO")
asymmetry_df = asymmetry_df.sort_values("TimestampISO")
```

# Heart Rate

## General Information

```{python}

#| label: Prettify_HR_Table

if has_hr:
    display(prettify_info(hr_df))
else:
    display(Markdown(f"_**No Heart Rate Data Exists**_"))

```

## Plotting

```{python}

#| label: "Plot_HR_Data_by_2-day_increments"
#| eval: False

if has_hr:
    origin = hr_df['TimestampISO'].dropna().min().normalize()

    hr_df["week_idx"] = ((hr_df["TimestampISO"] - origin) // pd.Timedelta(days=2)).astype(int)

    hr_df["WeekStart"] =origin + pd.to_timedelta(hr_df["week_idx"]*2, unit="D")

    weeks = sorted(hr_df["WeekStart"].unique())
    num_weeks = len(weeks)

    figs = go.Figure()

    # Add traces for each week, initially hidden
    for i, w in enumerate(weeks):
        # print(i)
        # print(w)
        temp = hr_df[hr_df["WeekStart"] == w].sort_values(by="TimestampISO")
        visible = [False] * len(weeks)
        visible[i] = True
        
        figs.add_trace(
            go.Scatter(
                x=temp["TimestampISO"],
                y=temp["HR"],
                mode="markers",
                name=f"Week of {w.date()}",
                marker=dict(color="red"),
                visible=(i == 0)  # only first visible by default
            )
        )

        figs.update_xaxes(range=[hr_df[hr_df["WeekStart"] == w].min(),hr_df[hr_df["WeekStart"] == w].max()] )

    # Dropdown menu
    buttons = [
        dict(
            label=f"Week of\n{w.date()}",
            method="update",
            args=[
                {"visible": [i == j for j in range(len(weeks))]},
                {"title": f"Week of\n{w.date()}"}
            ],
        )
        for i, w in enumerate(weeks)
    ]

    figs.update_layout(
        updatemenus=[{"buttons": buttons, "direction": "down"}],
        title=f"Heart Rate: Week of {weeks[0].date()}",
        template="plotly_white",
        yaxis_title="Beats Per Minute"
    )

    figs.show()
else:
    display(Markdown(f"_**No Heart Rate Data Exists**_"))
```

```{python}

#| eval: True

if has_hr:
    q = hr_df.sort_values(by="TimestampISO")

    fig = go.Figure()

    fig.add_trace(
    go.Scatter(
        x=q["TimestampISO"],
        y=q["HR"],
        mode="lines+markers",
        marker=dict(
        color="red"
        ),
        line=dict(
        color="green",
        width=1
        )
    )
    )

    # fig = px.line(
    #   x=q["TimestampISO"],
    #   y=q["HR"],
    #   markers=True
    #   )
    # fig.update_traces(
    #   line_color="red",
    # )
    fig.update_xaxes(
        rangeslider_visible=True,
        rangeselector=dict(
            buttons=list([
                dict(count=1, label="1 day", step="day", stepmode="backward"),
                dict(count=3, label="3 days", step="day", stepmode="backward"),
                dict(count=5, label="5 day", step="day", stepmode="backward"),
                dict(count=7, label="1 week", step="day", stepmode="backward"),
                dict(step="all")
            ])
        )
    )
    fig.show()
else:
    display(Markdown(f"_**No Heart Rate Data Exists**_"))

```

# Sleeping

## General Information

```{python}
#| eval: True
if has_sleep:
    display(prettify_info(sleep_df))
else:
    display(Markdown(f"_**No Sleep Data Exists**_"))
    # with open("MissingSleep.txt", "r") as file:
    # content = file.read()
    # print(content)

```

## Plotting

```{python}

#| eval: True

if has_sleep:
    sleep_df["Night_of_Sleep"] =  pd.to_datetime(sleep_df["Night_of_Sleep"], errors="coerce")

    origin = sleep_df['Night_of_Sleep'].min()

    sleep_df["week_idx"] = ((sleep_df["Night_of_Sleep"] - origin) / pd.Timedelta(days=7)).astype(int)

    sleep_df["WeekStart"] =origin + pd.to_timedelta(sleep_df["week_idx"]*7, unit="D")

    weeks = sorted(sleep_df["WeekStart"].unique())
    num_weeks = len(weeks)

    figs = go.Figure()

    # Add traces for each week, initially hidden
    for i, w in enumerate(weeks):
        temp = sleep_df[sleep_df["WeekStart"] == w].sort_values(by="Night_of_Sleep").drop_duplicates(subset=['EndISO'])
        visible = [False] * len(weeks)
        visible[i] = True
        
        figs.add_trace(
            go.Scatter(
                x=temp["Night_of_Sleep"],
                y=temp["Duration"].astype("int64")/60000000000,
                mode="markers",
                name=f"Week of {w.date()}",
                marker=dict(color="green"),
                visible=(i == 0)  # only first visible by default
            )
        )

        figs.update_xaxes(
        range=[
            temp["Night_of_Sleep"].min(),
            temp["Night_of_Sleep"].min() + pd.to_timedelta(7, unit="D"),
        ]
        )

        

    # Dropdown menu
    buttons = [
        dict(
            label=f"Sleep Data: Week of\n{w.date()}",
            method="update",
            args=[
                {"visible": [i == j for j in range(len(weeks))]},
                {"title": f"Week of\n{w.date()}"}
            ],
        )
        for i, w in enumerate(weeks)
    ]

    figs.update_layout(
        updatemenus=[{"buttons": buttons, "direction": "down"}],
        title=f"Week of {weeks[0].date()}",
        template="plotly_white",
        yaxis_title ="Minutes of Sleep"
    )
    figs.show()
else:
    display(Markdown(f"_**No Sleep Data Exists**_"))


```


# Gait and Walking Metrics

## General Information

```{python}

if has_gait:
    display(prettify_info(gait_df))
else:
    display(Markdown(f"_**No Gait Data Exists**_"))

```

## Plot


```{python}
#| eval: true
if has_gait:
    gait_df["TimestampISO"] = (
    pd.to_datetime(gait_df["TimestampISO"], utc=True, errors="coerce")
    .dt.tz_convert("America/Detroit")
    .infer_objects()
)   

    origin = gait_df['TimestampISO'].dropna().min().normalize()

    gait_df["week_idx"] = ((gait_df["TimestampISO"] - origin) // pd.Timedelta(days=2)).astype(int)

    gait_df["WeekStart"] =origin + pd.to_timedelta(gait_df["week_idx"]*2, unit="D")

    weeks = sorted(gait_df["WeekStart"].unique())
    num_weeks = len(weeks)

    figs = go.Figure()

    # Add traces for each week, initially hidden
    for i, w in enumerate(weeks):
        # print(i)
        # print(w)
        temp = gait_df[gait_df["WeekStart"] == w].sort_values(by="TimestampISO")
        visible = [False] * len(weeks)
        visible[i] = True
        
        figs.add_trace(
            go.Scatter(
                x=temp["TimestampISO"],
                y=temp["Balance"],
                mode="markers",
                name=f"Week of {w.date()}",
                marker=dict(color="blue"),
                visible=(i == 0)  # only first visible by default
            )
        )

        figs.update_xaxes(range=[gait_df[gait_df["WeekStart"] == w].min(),gait_df[gait_df["WeekStart"] == w].max()] )
        figs.update_yaxes(range=[0,1])

    # Dropdown menu
    buttons = [
        dict(
            label=f"Gait Data Week of\n{w.date()}",
            method="update",
            args=[
                {"visible": [i == j for j in range(len(weeks))]},
                {"title": f"Week of\n{w.date()}"}
            ],
        )
        for i, w in enumerate(weeks)
    ]

    figs.update_layout(
        yaxis_title="Gait Data",
        updatemenus=[{"buttons": buttons, "direction": "down"}],
        title=f"Balance: Week of {weeks[0].date()}",
        template="plotly_white"
    )

    figs.add_hline(y=0.5, line_dash="dash", line_color="green", annotation_text="Balanced Gait")
    figs.show()
else:
    display(Markdown(f"_**No Gait Data Exists**_"))
```

# Double Support

## General Information

```{python}

if has_double_support:
    display(prettify_info(double_support_df))
else:
    display(Markdown(f"_**No Gait Data Exists**_"))

```

## Plot


```{python}
#| eval: true
if has_double_support:
    double_support_df["TimestampISO"] = (
    pd.to_datetime(double_support_df["TimestampISO"], utc=True, errors="coerce")
    .dt.tz_convert("America/Detroit")
    .infer_objects()
)   

    origin = double_support_df['TimestampISO'].dropna().min().normalize()

    double_support_df["week_idx"] = ((double_support_df["TimestampISO"] - origin) // pd.Timedelta(days=2)).astype(int)

    double_support_df["WeekStart"] =origin + pd.to_timedelta(double_support_df["week_idx"]*2, unit="D")

    weeks = sorted(double_support_df["WeekStart"].unique())
    num_weeks = len(weeks)

    figs = go.Figure()

    # Add traces for each week, initially hidden
    for i, w in enumerate(weeks):
        # print(i)
        # print(w)
        temp = double_support_df[double_support_df["WeekStart"] == w].sort_values(by="TimestampISO")
        visible = [False] * len(weeks)
        visible[i] = True
        
        figs.add_trace(
            go.Scatter(
                x=temp["TimestampISO"],
                y=temp["Value"],
                mode="markers",
                name=f"Week of {w.date()}",
                marker=dict(color="blue"),
                visible=(i == 0)  # only first visible by default
            )
        )

        figs.update_xaxes(range=[double_support_df[double_support_df["WeekStart"] == w].min(),double_support_df[double_support_df["WeekStart"] == w].max()] )
        figs.update_yaxes(range=[0,1])

    # Dropdown menu
    buttons = [
        dict(
            label=f"Double Support Data Week of\n{w.date()}",
            method="update",
            args=[
                {"visible": [i == j for j in range(len(weeks))]},
                {"title": f"Week of\n{w.date()}"}
            ],
        )
        for i, w in enumerate(weeks)
    ]

    figs.update_layout(
        yaxis_title="Double Support Data",
        updatemenus=[{"buttons": buttons, "direction": "down"}],
        title=f"Balance: Week of {weeks[0].date()}",
        template="plotly_white"
    )

    figs.add_hline(y=0.5, line_dash="dash", line_color="green", annotation_text="Balanced Gait")
    figs.show()
else:
    display(Markdown(f"_**No Double Support Data Exists**_"))
```


# Merge Sleep and HR data

```{python}
#|label: Generate merged HR and Sleep dataframe 

earliest_time =  min(hr_df["TimestampISO"].min(), sleep_df["StartISO"].min())
latest_time = max(hr_df["TimestampISO"].max(), sleep_df["EndISO"].max())
time_df = pd.DataFrame({
    "GlobalTime": pd.date_range(
        start=earliest_time,
        end=latest_time,
        freq="30s"
    ),
    "Asleep": "0"
}
)

mask  = time_df["GlobalTime"].apply(
    lambda time: ((sleep_df["StartISO"] <= time) & (sleep_df["EndISO"] >= time)).any()
)

time_df["Asleep"] = mask.astype(int)
time_sorted_df = time_df.sort_values("GlobalTime")
hr_sorted_df = hr_df.sort_values("TimestampISO")

merge = pd.merge_asof(
    time_sorted_df,
    hr_sorted_df,
    left_on="GlobalTime",
    right_on="TimestampISO",
    direction="nearest",
    tolerance=pd.Timedelta(seconds=15)
)

new_order = merge[['ID','GlobalTime', 'Asleep', 'HR']]
```


```{python}
IDs = []
night = []
start = []
end = []
average_hr = []
sd_hr = []

for index, _ in sleep_df.iterrows():
    ID = sleep_df["ID"][index]
    start_time  = sleep_df["StartISO"][index]
    end_time  = sleep_df["EndISO"][index]
    sleep_day = sleep_df["Night_of_Sleep"][index]
    IDs.append(ID)
    start.append(start_time)
    end.append(end_time)
    night.append(sleep_day)

    aveHR = hr_sorted_df["HR"].where((hr_sorted_df["TimestampISO"] >= start_time) &(hr_sorted_df["TimestampISO"] < end_time)).mean()
    std_hr = hr_sorted_df["HR"].where((hr_sorted_df["TimestampISO"] >= start_time) &(hr_sorted_df["TimestampISO"] < end_time)).std()
    average_hr.append(aveHR)
    sd_hr.append(std_hr)
    # new_df["AverageHR"] = new_order["HR"].where(
    #     (new_order["GlobalTime"] >= start_time)
    # ).mean()

hr_dictionary = {
    'ID': IDs,
    'Night_of_Sleep': night,
    'Start_Time': start,
    'End_Time': end,
    'Mean_HR_Present_During_Period':average_hr,
    'Standard_Deviation_of_Present_HR_During Period': sd_hr
}

HR_summary_df = pd.DataFrame(hr_dictionary)

csv_name = "HR_Summary_"+RID+".csv"

HR_summary_df.to_csv(csv_name)
```


```{python}
prettify_info(HR_summary_df)

```




```{python}
new_presence_row=pd.DataFrame([{
    "ID": RID,
    "HR_Present": has_hr,
    "Sleep_Present": has_sleep,
    "Double_Support_Present": has_double_support,
    "Asymmetry_Present": has_asymmetry 
}])


presence_df = pd.concat([presence_df, new_presence_row], ignore_index=True)

presence_df = presence_df.drop_duplicates(subset=["ID"], keep="last").to_csv(os.path.join(PathKeeper.data_log_path,"Data_Presence_Log.csv"), index=False)


```