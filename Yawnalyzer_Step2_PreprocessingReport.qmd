---
title: "Yawnalyzer"
subtitle: "Preprocessing Report"
author:
  - name: NAME
    email: EMAIL
    affiliations: AFFILIATION
date: last-modified
date-format: "[Last Modified:] DD MMM YYYY"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    df-print: paged
    embed-resources: true
    html-math-method: mathjax
jupyter: python3
---

```{python}

#| tags: [parameters]
#| echo: false
#| message:
#| code-fold: false

RID = "PARTICIPANT_ID"

```

```{python}

#| label: "Import_Packages" 
#| echo: false
#| message:
#| code-fold: false

import os
import pandas as pd
import numpy as np
#import neurokit2 as nk
import plotly.express as px
import plotly.graph_objects as go
import datetime as dt
from datetime import datetime, timedelta
from IPython.display import Markdown, display
import session_info
import PathKeeper

```

# About this Report

```{python}

#| label: ParticipantID
#| code-hide: true
#| echo: false



Markdown(f"> Participant: _**{RID}**_")

```

> Generation Date: `{python} dt.datetime.now().strftime("%Y-%m-%d")`

This is a processing report for Subject ID (SID) **`{python} RID`** from the MS-EMA project (IRB #HUM00265263; PI: Tiffany Braley, MD). MS-EMA is an Apple watch-based sleep tracking study conducted with patients with Multiple Sclerosis. 

This report provides both a sense of quality and summary metrics for the following types of information:

- Heart rate (derived from PPG)
- Sleep-related metrics
- Gait and balance

Data for the above were collected over the span of approximately 12 weeks for each subject. Plots contained within this report are summarized by week.


```{python}

#| label: "Functions"
#| echo: false
#| warning: false
#| message: false

def prettify_info(df, memory_usage=False, deep=True, sort_by_nulls=False, show_unique=True, show_stats=True):
  n=len(df)
  info_df=pd.DataFrame({
    
    "Column":df.columns,
    "Total Count" : df.count(),
    "Not-Nulls Count" : df.notnull().sum(),
    "Missing Count" : df.isnull().sum(),
    "Type" : df.dtypes.astype(str)
    })
  if show_unique:
    info_df["Unique Count"] = df.nunique(dropna=True)
  
  if show_stats:
    numeric_cols = df.select_dtypes(include=[np.number])

    sum_stats_dictionary ={
        "Means" : numeric_cols.mean(),#.round(5)
        "Median" : numeric_cols.median(),#.round(5)
        "Standard Deviation" : numeric_cols.std(),#.round(5)
        "Min": numeric_cols.min(),
        "25%": numeric_cols.quantile(0.25),
        "75%": numeric_cols.quantile(0.75),
        "Max": numeric_cols.max()
    }

    for stat_name, stat_values in sum_stats_dictionary.items():
        valid_numeric_cols = set(stat_values.index)

        info_df[stat_name] = info_df["Column"].apply(
            lambda col: stat_values[col] if col in valid_numeric_cols else "NA"
        )

  base_cols = ["Total Count","Not-Nulls Count","Missing Count"]
  uniqueness_cols = ["Unique Count"] if show_unique else []
  stat_cols = list(sum_stats_dictionary.keys()) if show_stats else []
  info_df = info_df[base_cols + uniqueness_cols + stat_cols + ["Type"]]

  if memory_usage:
    mem = df.memory_usage(deep=deep).sum / 1024**2
    print(f"Memory usage: {mem.df} MB")

  
  return info_df

```



```{python}

#| label: "Inital-Read"
#| echo: false
#| warning: false
#| message: false

presence_df = pd.read_csv(os.path.join(PathKeeper.data_log_path,"Data_Presence_Log.csv"))

hr_df = pd.read_csv(os.path.join(PathKeeper.merged_data_path,"hr_collapsed.csv"), index_col=["index_1"], dtype={
  "ID": "string",
  "submission_date": "string",
  "file_type": "string",
  "HR": "float64",
  "Timestamp":  "float64",
  "filename": "string" 
 }
)

sleep_df = pd.read_csv(os.path.join(PathKeeper.merged_data_path,"sleep_collapsed.csv"), index_col=["index_1"])

gait_df = pd.read_csv(os.path.join(PathKeeper.merged_data_path,"gait_collapsed.csv"), index_col=["index_1"])

double_support_df = pd.read_csv(os.path.join(PathKeeper.merged_data_path,"double_support_collapsed.csv"), index_col=["index_1"])

asymmetry_df =pd.read_csv(os.path.join(PathKeeper.merged_data_path,"asymmetry_collapsed.csv"), index_col=["index_1"])

fatigue_survey_df = pd.read_csv(os.path.join(PathKeeper.merged_data_path, "fatigue_survey_collapsed.csv"), index_col=["index_1"]
)

cognitive_survey_df = pd.read_csv(os.path.join(PathKeeper.merged_data_path, "cog_collapsed.csv"), index_col=["index_1"]
)

sleep_survey_df = pd.read_csv(os.path.join(PathKeeper.merged_data_path, "sleep_survey_collapsed.csv"), index_col=["index_1"]
)

hr_df = hr_df[hr_df["ID"] == RID].copy()
sleep_df = sleep_df[sleep_df["ID"] == RID].copy()
gait_df = gait_df[gait_df["ID"] == RID].copy()
double_support_df = double_support_df[double_support_df["ID"] == RID].copy()
asymmetry_df = asymmetry_df[asymmetry_df["ID"] == RID].copy()
sleep_survey_df = sleep_survey_df[sleep_survey_df["ID"] == RID].copy()
cognitive_survey_df = cognitive_survey_df[cognitive_survey_df["ID"] == RID].copy()
fatigue_survey_df = fatigue_survey_df[fatigue_survey_df["ID"] == RID].copy()

all_surveys_df = sleep_survey_df.drop("Question4", axis=1).merge(cognitive_survey_df, how="outer")

all_surveys_df = all_surveys_df.merge(fatigue_survey_df.drop("Question4", axis=1), how="outer").drop('DATE_KEY,ANSWERS', axis=1)

all_surveys_df["Sleep03"] = all_surveys_df["Sleep03"].str.strip().replace("Sleep03: No new symptoms","Sleep03: 0 = No new symptoms")


all_surveys_df[["survey_date", "survey_type"]] = all_surveys_df["Survey"].str.rsplit("_", n=1, expand=True)

cols = ['Sleep01', 'Sleep02', 'Sleep03', 'Cognitive01','Cognitive02', "Pain",'Depression', 'Physical_Fatigue', 'Brain_Fatigue', 'Sleepiness']


for c in cols:
    col_as_series = all_surveys_df[c].fillna("").astype(str)
    non_missing_index = col_as_series.str.contains(":")
    non_missing_series = col_as_series.str.split(":", n=1, expand=True)
    non_missing_df = non_missing_series.reindex(columns=[0,1])
    non_missing_df = non_missing_df.apply(lambda col: col.str.strip())

    all_surveys_df[f"{c}_question"] = np.where(
        non_missing_index,
        non_missing_df[0],
        f"{c}_question"
    )  

    all_surveys_df[f"{c}_value"] = np.where(
        non_missing_index,
        non_missing_df[1],
        np.nan
    )
    
    all_surveys_df[f"{c}_value"] = ( all_surveys_df[f"{c}_value"].astype(str).str.extract(r"(\d+)").astype("Int64"))

all_surveys_df = all_surveys_df.drop(['Sleep01', 'Sleep02', 'Sleep03', 'Cognitive01','Cognitive02','Pain','Depression', 'Physical_Fatigue', 'Brain_Fatigue', 'Sleepiness'], axis=1)

cols_to_drop = [col for col in all_surveys_df.columns if "_question" in col]

all_surveys_df = all_surveys_df.drop(columns=cols_to_drop)
all_surveys_df.columns = all_surveys_df.columns.str.replace("_value","")

cognitive_survey_df = all_surveys_df[all_surveys_df["survey_type"] == "Cognitive"].dropna(axis=1, how="all").drop(columns=["survey_type"])

fatigue_survey_df = all_surveys_df[all_surveys_df["survey_type"] == "Fatigue"].dropna(axis=1, how="all").drop(columns=["survey_type"])

sleep_survey_df  = all_surveys_df[all_surveys_df["survey_type"] == "Sleep"].dropna(axis=1, how="all").drop(columns=["survey_type"])

has_hr ="hr_df" in locals() and len(hr_df) > 0

has_sleep ="sleep_df" in locals() and len(sleep_df) > 0

has_gait ="gait_df" in locals() and len(gait_df) > 0

has_double_support ="double_support_df" in locals() and len(double_support_df) > 0

has_asymmetry= "asymmetry_df" in locals() and len(asymmetry_df) > 0

has_cognitive_survey= "cognitive_survey_df" in locals() and len(cognitive_survey_df) > 0
has_fatique_survey= "fatigue_survey_df" in locals() and len(fatigue_survey_df) > 0
has_sleep_survey= "sleep_survey_df" in locals() and len(sleep_survey_df) > 0

overall_summary = {}

overall_summary["ID"] =  RID;

del non_missing_df, non_missing_index, non_missing_series, col_as_series, cols, cols_to_drop, c
```

```{python}

#| label: "Time-Conversions"
#| eval: True
#| echo: false
#| warning: false
#| message: false

#Sleep 

sleep_df["StartISO"] = pd.to_datetime(sleep_df["StartISO"], utc=True).dt.tz_convert(
    "America/Detroit"
)

sleep_df["EndISO"] = pd.to_datetime(sleep_df["EndISO"], utc=True).dt.tz_convert(
    "America/Detroit"
)

sleep_df["StartISO"] = (
    sleep_df["StartISO"]
    .infer_objects()
    .fillna(pd.to_datetime(sleep_df["Start"], unit="s", utc=True))
)

sleep_df["EndISO"] = (
    sleep_df["EndISO"]
    .infer_objects()
    .fillna(pd.to_datetime(sleep_df["End"], unit="s", utc=True))
)

sleep_df = sleep_df.sort_values(by="StartISO")

sleep_df["gap_seconds"]  = (
    sleep_df.groupby("ID")["EndISO"].shift().rsub(sleep_df["StartISO"]).dt.total_seconds()
    )
sleep_df["gap_hours"] = (sleep_df["gap_seconds"]/3600) 

sleep_df["Session"] = sleep_df["gap_hours"].isna() | (sleep_df["gap_hours"] >=2)

sleep_df["session_id"] = sleep_df["Session"].cumsum()

sleep_df = (
    sleep_df.groupby(["session_id",'ID', 'submission_date', 'file_type', 'filename']).agg(
        ID = ("ID", "first"),
        submission_date = ('submission_date', "first"), 
        file_type = ('file_type',"first"),
        filename=('filename', 'first'),
        StartISO =('StartISO', "min"),
        EndISO = ('EndISO', "max"),
        Start = ('Start', "min"),
        End = ('End', "max")
    )
    .reset_index(drop=True)
)

sleep_df["Duration"] = sleep_df["EndISO"] - sleep_df["StartISO"]

sleep_df["hour"] = sleep_df["EndISO"].dt.hour

sleep_df["Before_or_After_noon"] = np.where(sleep_df["EndISO"].dt.hour < 12,"Morning","Afternoon")

sleep_df["Night_of_Sleep"] = np.where(
    sleep_df["Before_or_After_noon"] == "Morning",
    (sleep_df["EndISO"] - pd.Timedelta(days=1)).dt.date,
    sleep_df["EndISO"].dt.date,
)

sleep_df["Duration"] = sleep_df["Duration"].where(
    sleep_df["Duration"] <= pd.Timedelta(hours=24),
)

sleep_df["Night_of_Sleep"] = sleep_df["Night_of_Sleep"].where(
    sleep_df["Duration"].notna()
)

sleep_df["Night_of_Sleep"] = pd.to_datetime(
    sleep_df["Night_of_Sleep"], utc=True,
    errors="coerce").dt.tz_convert("America/Detroit")

#sleep_df["Night_of_Sleep"] = sleep_df["Night_of_Sleep"].dt.date

sleep_df["EndISO"] = (
    sleep_df["EndISO"]
    .infer_objects()
    .fillna(pd.to_datetime(sleep_df["End"], unit="s", utc=True))
)

sleep_df=sleep_df.drop(["hour","Before_or_After_noon"], axis=1)


#Heart Rate

hr_df["TimestampISO"] = pd.to_datetime(hr_df["TimestampISO"],errors="coerce", utc=True)


hr_df["TimestampISO"] = (
    hr_df["TimestampISO"]
    .infer_objects()
    .fillna(pd.to_datetime(hr_df["Timestamp"], unit="s", utc=True))
)

hr_df["TimestampISO"] = (
    pd.to_datetime(hr_df["TimestampISO"], utc=True, errors="coerce")
    .dt.tz_convert("America/Detroit")
    .infer_objects()
)   

# Gait
gait_df["TimestampISO"] = (
    gait_df["TimestampISO"]
    .infer_objects()
    .fillna(pd.to_datetime(gait_df["Timestamp"], unit="s", utc=False))
)

# Double Support
double_support_df["TimestampISO"] = (
    double_support_df["TimestampISO"]
    .infer_objects()
    .fillna(pd.to_datetime(double_support_df["Timestamp"], unit="s", utc=False))
)

# Asymmetry

asymmetry_df["TimestampISO"] = (
    asymmetry_df["TimestampISO"]
    .infer_objects()
    .fillna(pd.to_datetime(asymmetry_df["Timestamp"], unit="s", utc=False))
)

# Cognitive survey

cognitive_survey_df["Timestamp"] = pd.to_datetime(cognitive_survey_df["Timestamp"],errors="coerce", utc=True).dt.tz_convert(
    "America/Detroit"
)

# Fatigue survey

fatigue_survey_df["Timestamp"] = pd.to_datetime(fatigue_survey_df["Timestamp"],errors="coerce", utc=True).dt.tz_convert(
    "America/Detroit"
)

# Sleep survey

sleep_survey_df["Timestamp"] = pd.to_datetime(sleep_survey_df["Timestamp"],errors="coerce", utc=True).dt.tz_convert(
    "America/Detroit"
)



```

```{python}
#|label: Sort dataframes by Time
#| echo: false
#| warning: false
#| message: false

hr_df=hr_df.sort_values(by="TimestampISO")
sleep_df = sleep_df.sort_values(by="StartISO")
gait_df = gait_df.sort_values(by="TimestampISO")
double_support_df = double_support_df.sort_values("TimestampISO")
asymmetry_df = asymmetry_df.sort_values("TimestampISO")
cognitive_survey_df = cognitive_survey_df.sort_values("Timestamp")
fatigue_survey_df = fatigue_survey_df.sort_values("Timestamp") 
sleep_survey_df = sleep_survey_df.sort_values("Timestamp")

```

# Heart Rate

## General Information

```{python}

#| label: Prettify_HR_Table
#| echo: false
#| warning: false
#| message: false

if has_hr:
    display(prettify_info(hr_df))
else:
    display(Markdown(f"_**No Heart Rate Data Exists**_"))

```

## Plotting

```{python}

#| label: "Plot_HR_Data_by_2-day_increments"
#| eval: False
#| echo: false
#| warning: false
#| message: false

if has_hr:
    origin = hr_df['TimestampISO'].dropna().min().normalize()

    hr_df["week_idx"] = ((hr_df["TimestampISO"] - origin) // pd.Timedelta(days=7)).astype(int)

    hr_df["WeekStart"] =origin + pd.to_timedelta(hr_df["week_idx"]*7, unit="D")

    weeks = sorted(hr_df["WeekStart"].unique())
    num_weeks = len(weeks)

    figs = go.Figure()
    week_ranges = {
        w:[w, w+pd.Timedelta(days=7)]
        for w in weeks
    }

    # Add traces for each week, initially hidden
    for i, w in enumerate(weeks):
        # print(i)
        # print(w)
        temp = hr_df[hr_df["WeekStart"] == w].sort_values(by="TimestampISO")
        visible = [False] * len(weeks)
        visible[i] = True
        
        figs.add_trace(
            go.Scatter(
                x=temp["TimestampISO"],
                y=temp["HR"],
                mode="markers",
                name=f"Week of {w.date()}",
                marker=dict(color="red"),
                visible=(i == 0)  # only first visible by default
            )
        )

        figs.update_xaxes(range=[hr_df[hr_df["WeekStart"] == w].min(),hr_df[hr_df["WeekStart"] == w].max()] )

    # Dropdown menu
    buttons = [
        dict(
            label=f"Week of\n{w.date()}",
            method="update",
            args=[
                {"visible": [i == j for j in range(len(weeks))]},
                {"title": f"Week of\n{w.date()}"}
            ],
        )
        for i, w in enumerate(weeks)
    ]

    figs.update_layout(
        updatemenus=[{"buttons": buttons, "direction": "down"}],
        title=f"Heart Rate: Week of {weeks[0].date()}",
        template="plotly_white",
        yaxis_title="Beats Per Minute"
    )

    figs.show()
else:
    display(Markdown(f"_**No Heart Rate Data Exists**_"))
```

```{python}

#| eval: True
#| echo: false
#| warning: false
#| message: false

if has_hr:
    q = hr_df.sort_values(by="TimestampISO")

    fig = go.Figure()

    fig.add_trace(
    go.Scatter(
        x=q["TimestampISO"],
        y=q["HR"],
        mode="lines+markers",
        marker=dict(
        color="red"
        ),
        line=dict(
        color="green",
        width=1
        )
    )
    )

    # fig = px.line(
    #   x=q["TimestampISO"],
    #   y=q["HR"],
    #   markers=True
    #   )
    # fig.update_traces(
    #   line_color="red",
    # )
    fig.update_xaxes(
        rangeslider_visible=True,
        rangeselector=dict(
            buttons=list([
                dict(count=1, label="1 day", step="day", stepmode="backward"),
                dict(count=3, label="3 days", step="day", stepmode="backward"),
                dict(count=5, label="5 day", step="day", stepmode="backward"),
                dict(count=7, label="1 week", step="day", stepmode="backward"),
                dict(step="all")
            ])
        )
    )
    fig.show()
else:
    display(Markdown(f"_**No Heart Rate Data Exists**_"))

```

# Sleeping

## General Information

```{python}
#| eval: True
#| echo: false
#| warning: false
#| message: false

if has_sleep:
    display(prettify_info(sleep_df))
else:
    display(Markdown(f"_**No Sleep Data Exists**_"))
    # with open("MissingSleep.txt", "r") as file:
    # content = file.read()
    # print(content)

```

## Plotting

Below is a stacked bargraph. Each bar on the x-axis represents a different potential night of sleep that occurred between the start and end of their participation. If they had multiple sleep sessison for a given night of sleep, each session is a different color, and are stacked.

The y-axis ths the duration of the recorded sleep session in minutes.

The blue line represents 8 hours of sleep.

```{python}

#| label: Sleep_Duration_Plot
#| eval: True
#| echo: false
#| warning: false
#| message: false

if has_sleep:
    sleep_temp_df = sleep_df.sort_values(by="Night_of_Sleep")
    sleep_temp_df["Duration"] = (sleep_temp_df["Duration"]/60_000_000_000)
    sleep_temp_df['Sleep Session'] = sleep_temp_df.groupby('Night_of_Sleep').cumcount()
    sleep_temp_df["Sleep Session"] = "Session: " + (sleep_temp_df["Sleep Session"]+1).astype(str) 

    fig = go.Figure()
    fig.add_hline(y=(60*8), line_dash="dash", line_color="cyan", layer='below')

    for session in sleep_temp_df["Sleep Session"].unique():
        filtered_df = sleep_temp_df[sleep_temp_df["Sleep Session"] == session]

        fig.add_trace(
            go.Bar(
                x=filtered_df["Night_of_Sleep"],
                y=filtered_df["Duration"],
                name=session     # ← now a proper string label
            )
        )
    fig.update_layout(
        xaxis_title = "Night of Sleep",
        yaxis_title = "Duration of Sleep (in Minutes)",
        barmode="stack",
        showlegend=False
    )

    fig.update_xaxes(
        rangeslider_visible=True,
        rangeselector=dict(
            buttons=list([
                dict(count=1, label="1 day", step="day", stepmode="backward"),
                dict(count=3, label="3 days", step="day", stepmode="backward"),
                dict(count=5, label="5 day", step="day", stepmode="backward"),
                dict(count=7, label="1 week", step="day", stepmode="backward"),
                dict(step="all")
            ])
        )
    )
    fig.show()
else:
    display(Markdown(f"_**No Sleep Data Exists**_"))


```


# Gait and Walking Metrics

## Gait

### General Information

```{python}
#| eval: True
#| echo: false
#| warning: false
#| message: false

if has_gait:
    display(prettify_info(gait_df))
else:
    display(Markdown(f"_**No Gait Data Exists**_"))

```

### Plot


```{python}
#| eval: true
#| echo: false
#| warning: false
#| message: false


if has_gait:
    gait_df["TimestampISO"] = (
    pd.to_datetime(gait_df["TimestampISO"], utc=True, errors="coerce")
    .dt.tz_convert("America/Detroit")
    .infer_objects()
)   

    origin = gait_df['TimestampISO'].dropna().min().normalize()

    gait_df["week_idx"] = ((gait_df["TimestampISO"] - origin) // pd.Timedelta(days=7)).astype(int)

    gait_df["WeekStart"] =origin + pd.to_timedelta(gait_df["week_idx"]*7, unit="D")

    weeks = sorted(gait_df["WeekStart"].unique())
    num_weeks = len(weeks)

    figs = go.Figure()

    # Add traces for each week, initially hidden
    for i, w in enumerate(weeks):
        # print(i)
        # print(w)
        temp = gait_df[gait_df["WeekStart"] == w].sort_values(by="TimestampISO")
        visible = [False] * len(weeks)
        visible[i] = True
        
        figs.add_trace(
            go.Scatter(
                x=temp["TimestampISO"],
                y=temp["Balance"],
                mode="markers",
                name=f"Week of {w.date()}",
                marker=dict(color="blue"),
                visible=(i == 0)  # only first visible by default
            )
        )

        figs.update_xaxes(range=[gait_df[gait_df["WeekStart"] == w].min(),gait_df[gait_df["WeekStart"] == w].max()] )
        figs.update_yaxes(range=[0,1])

    # Dropdown menu
    buttons = [
        dict(
            label=f"Gait Data Week of\n{w.date()}",
            method="update",
            args=[
                {"visible": [i == j for j in range(len(weeks))]},
                {"title": f"Week of\n{w.date()}"}
            ],
        )
        for i, w in enumerate(weeks)
    ]

    figs.update_layout(
        yaxis_title="Gait Data",
        updatemenus=[{"buttons": buttons, "direction": "down"}],
        title=f"Balance: Week of {weeks[0].date()}",
        template="plotly_white"
    )

    figs.add_hline(y=0.5, line_dash="dash", line_color="green", annotation_text="Balanced Gait")
    figs.show()
else:
    display(Markdown(f"_**No Gait Data Exists**_"))
```

## Double Support

### General Information

```{python}

#| eval: True
#| echo: false
#| warning: false
#| message: false

if has_double_support:
    display(prettify_info(double_support_df))
else:
    display(Markdown(f"_**No Double Support Data Exists**_"))

```

### Plot


```{python}

#| eval: True
#| echo: false
#| warning: false
#| message: false

if has_double_support:
    double_support_df["TimestampISO"] = (
    pd.to_datetime(double_support_df["TimestampISO"], utc=True, errors="coerce")
    .dt.tz_convert("America/Detroit")
    .infer_objects()
)   

    origin = double_support_df['TimestampISO'].dropna().min().normalize()

    double_support_df["week_idx"] = ((double_support_df["TimestampISO"] - origin) // pd.Timedelta(days=7)).astype(int)

    double_support_df["WeekStart"] =origin + pd.to_timedelta(double_support_df["week_idx"]*7, unit="D")

    weeks = sorted(double_support_df["WeekStart"].unique())
    num_weeks = len(weeks)

    figs = go.Figure()

    # Add traces for each week, initially hidden
    for i, w in enumerate(weeks):
        # print(i)
        # print(w)
        temp = double_support_df[double_support_df["WeekStart"] == w].sort_values(by="TimestampISO")
        visible = [False] * len(weeks)
        visible[i] = True
        
        figs.add_trace(
            go.Scatter(
                x=temp["TimestampISO"],
                y=temp["Percentage"],
                mode="markers",
                name=f"Week of {w.date()}",
                marker=dict(color="blue"),
                visible=(i == 0)  # only first visible by default
            )
        )

        figs.update_xaxes(range=[double_support_df[double_support_df["WeekStart"] == w].min(),double_support_df[double_support_df["WeekStart"] == w].max()] )
        figs.update_yaxes(range=[0,1])

    # Dropdown menu
    buttons = [
        dict(
            label=f"Double Support Data Week of\n{w.date()}",
            method="update",
            args=[
                {"visible": [i == j for j in range(len(weeks))]},
                {"title": f"Week of\n{w.date()}"}
            ],
        )
        for i, w in enumerate(weeks)
    ]

    figs.update_layout(
        yaxis_title="Percentage of Time with Both Feet on Ground",
        updatemenus=[{"buttons": buttons, "direction": "down"}],
        title=f"Double Support: Week of {weeks[0].date()}",
        template="plotly_white"
    )
    figs.show()
else:
    display(Markdown(f"_**No Double Support Data Exists**_"))
```


# Surveys

## Cognitive
### General Information

```{python}

#| eval: True
#| echo: false
#| warning: false
#| message: false

if has_cognitive_survey:
    display(prettify_info(cognitive_survey_df))
else:
    display(Markdown(f"_**No Cognitive Survey Responses Exists**_"))

```

### Plotting

```{python}

#| eval: True
#| echo: false
#| warning: false
#| message: false

cognitive_survey_long_df = cognitive_survey_df.melt(id_vars=["ID","survey_date", "Timestamp"], value_vars=["Cognitive01","Cognitive02","Pain", "Depression"], var_name='Questions', value_name='Value')

cognitive_survey_long_df["survey_date"] = (
    pd.to_datetime(cognitive_survey_long_df["survey_date"], format="%Y-%m-%d_%H%M").dt.tz_localize("America/New_York")
)

####

if has_cognitive_survey:
    cognitive_survey_long_df = cognitive_survey_long_df.sort_values(["survey_date", "Timestamp"])
    fig = px.line(
        cognitive_survey_long_df,
        x="survey_date",
        y="Value",
        color="Questions",
        facet_row="Questions",
        markers=True
    )
    fig.for_each_annotation(
    lambda a: a.update(text=a.text.split("=")[-1])
    
    )

    # 1. Match all x-axes
    fig.update_xaxes(matches="x")

    # 2. Disable sliders everywhere
    fig.update_xaxes(rangeslider_visible=False)

    # 3. Enable slider ONLY on the bottom x-axis
    bottom_xaxis = min(
        fig.select_xaxes(),
        key=lambda ax: ax.domain[0]
    )
    bottom_xaxis.rangeslider.visible = True
    fig.update_layout(showlegend=False)



    fig.show()
else:
    display(Markdown(f"_**No Cognitvie Survey Data Exists**_"))
```

## Fatigue
### General Information

```{python}

#| eval: True
#| echo: false
#| warning: false
#| message: false

if has_fatique_survey:
    display(prettify_info(fatigue_survey_df))
else:
    display(Markdown(f"_**No Fatigue Survey Responses Exists**_"))

```

### Plotting

```{python}

#| eval: True
#| echo: false
#| warning: false
#| message: false

fatigue_survey_long_df = fatigue_survey_df.melt(id_vars=["ID","survey_date", "Timestamp"], value_vars=[ 'Physical_Fatigue', 'Brain_Fatigue', 'Sleepiness'], var_name='Questions', value_name='Value')

fatigue_survey_long_df["survey_date"] = (
    pd.to_datetime(fatigue_survey_long_df["survey_date"], format="%Y-%m-%d_%H%M").dt.tz_localize("America/New_York")
)

####

if has_fatique_survey:
    fatigue_survey_long_df=fatigue_survey_long_df.sort_values(["survey_date", "Timestamp"])
    fig = px.line(
        fatigue_survey_long_df,
        x="survey_date",
        y="Value",
        color="Questions",
        facet_row="Questions",
        markers=True,
    )
    fig.for_each_annotation(
    lambda a: a.update(text=a.text.split("=")[-1])
    
    )
    
    fig.update_xaxes(matches="x")

    # 2. Disable sliders everywhere
    fig.update_xaxes(rangeslider_visible=False)

    # 3. Enable slider ONLY on the bottom x-axis
    bottom_xaxis = min(
        fig.select_xaxes(),
        key=lambda ax: ax.domain[0]
    )
    fig.update_layout(showlegend=False)

    bottom_xaxis.rangeslider.visible = True


    fig.show()
else:
    display(Markdown(f"_**No Fatigue Survey Data Exists**_"))
```

# Merge Sleep and HR data

```{python}
#| label: Generate merged HR and Sleep dataframe 
#| eval: True
#| echo: false
#| warning: false
#| message: false

earliest_time =  min(hr_df["TimestampISO"].min(), sleep_df["StartISO"].min())
latest_time = max(hr_df["TimestampISO"].max(), sleep_df["EndISO"].max())
time_df = pd.DataFrame({
    "GlobalTime": pd.date_range(
        start=earliest_time,
        end=latest_time,
        freq="30s"
    ),
    "Asleep": "0"
}
)

mask  = time_df["GlobalTime"].apply(
    lambda time: ((sleep_df["StartISO"] <= time) & (sleep_df["EndISO"] >= time)).any()
)

time_df["Asleep"] = mask.astype(int)
time_sorted_df = time_df.sort_values("GlobalTime")
hr_sorted_df = hr_df.sort_values("TimestampISO")

merge = pd.merge_asof(
    time_sorted_df,
    hr_sorted_df,
    left_on="GlobalTime",
    right_on="TimestampISO",
    direction="nearest",
    tolerance=pd.Timedelta(seconds=15)
)

new_order = merge[['ID','GlobalTime', 'Asleep', 'HR']]
```


```{python}

#| eval: True
#| echo: false
#| warning: false
#| message: false

IDs = []
night = []
start = []
end = []
average_hr = []
sd_hr = []
ealiest_hr =[]
latest_hr =[]
earliest_timestamp=[]
latest_timestamp = []

for index, _ in sleep_df.iterrows():
    ID = sleep_df["ID"][index]
    start_time  = sleep_df["StartISO"][index]
    end_time  = sleep_df["EndISO"][index]
    sleep_day = sleep_df["Night_of_Sleep"][index]
    IDs.append(ID)
    start.append(start_time)
    end.append(end_time)
    night.append(sleep_day)

    aveHR = hr_sorted_df["HR"].where((hr_sorted_df["TimestampISO"] >= start_time) &(hr_sorted_df["TimestampISO"] < end_time)).mean()
    std_hr = hr_sorted_df["HR"].where((hr_sorted_df["TimestampISO"] >= start_time) &(hr_sorted_df["TimestampISO"] < end_time)).std()
    
    first_hr_timestamp = hr_sorted_df["TimestampISO"].where((hr_sorted_df["TimestampISO"] >= start_time) &(hr_sorted_df["TimestampISO"] < end_time)&(hr_sorted_df["HR"].notna())).iloc[0],
    last_hr_timestamp = hr_sorted_df["TimestampISO"].where((hr_sorted_df["TimestampISO"] >= start_time) &(hr_sorted_df["TimestampISO"] < end_time)&(hr_sorted_df["HR"].notna())).iloc[-1]
    
    first_hr = hr_sorted_df["HR"].where((hr_sorted_df["TimestampISO"] >= start_time) &(hr_sorted_df["TimestampISO"] < end_time)&(hr_sorted_df["HR"].notna())).iloc[0]
    last_hr = hr_sorted_df["HR"].where((hr_sorted_df["TimestampISO"] >= start_time) &(hr_sorted_df["TimestampISO"] < end_time)&(hr_sorted_df["HR"].notna())).iloc[-1]

    average_hr.append(aveHR)
    sd_hr.append(std_hr)
    ealiest_hr.append(first_hr)
    latest_hr.append(last_hr)
    earliest_timestamp.append(first_hr_timestamp)
    latest_timestamp.append(last_hr_timestamp)
    # new_df["AverageHR"] = new_order["HR"].where(
    #     (new_order["GlobalTime"] >= start_time)
    # ).mean()

hr_dictionary = {
    'ID': IDs,
    'Night_of_Sleep': night,
    'Start_Time': start,
    'End_Time': end,
    'Mean_HR_Present_During_Period':average_hr,
    'Standard_Deviation_of_Present_HR_During Period': sd_hr,
    'Earliest_Recorded_HR': ealiest_hr,
    'Earliest_Recorded_HR_Timestamp':earliest_timestamp,
    'Latest_Recorded_HR': latest_hr,
    'Latest_Recorded_HR_Timestamp': latest_timestamp,
}

HR_summary_df = pd.DataFrame(hr_dictionary)

csv_name = "HR_Summary_"+RID+".csv"

HR_summary_df.dropna(subset=["Night_of_Sleep"]).to_csv(os.path.join(PathKeeper.summarized_data_path,csv_name), index=False)
```

```{python}

#| eval: True
#| echo: false
#| warning: false
#| message: false

earliest_day =  sleep_df["Night_of_Sleep"].min()
latest_day = sleep_df["Night_of_Sleep"].max()

sleep_days_df = pd.DataFrame({
    "GlobalDay": pd.date_range(
        start=earliest_day,
        end=latest_day,
        freq="1D"
    )
}
)

sleep_df2 = sleep_df.dropna(subset=["Night_of_Sleep"]).copy()

sleep_df2["Night_of_Sleep"] = pd.to_datetime(sleep_df["Night_of_Sleep"])

sleep_all_df = pd.merge(sleep_days_df, sleep_df2, left_on="GlobalDay", right_on="Night_of_Sleep", how="left").sort_values(["GlobalDay", "StartISO"])

sleep_all_df["ID"] = sleep_all_df["ID"].fillna(RID)

sleep_summary_df = pd.DataFrame()

for date, dataframe in sleep_all_df.groupby("GlobalDay"):
    if dataframe["Duration"].isna().all():
        dataframe["Asleep_Time"] = pd.NA
        dataframe["AwakeTime"] = pd.NA
        dataframe["Date"] = date
    else:
        dataframe["Asleep_Time"] = dataframe["Duration"].sum()
        dataframe["Awake_Time"] =  dt.timedelta(days=1) -dataframe["Asleep_Time"]
        dataframe["Date"] = date
    sleep_summary_df = pd.concat([sleep_summary_df, dataframe], ignore_index=True) 

sleep_summary_df = sleep_summary_df[["ID", "GlobalDay", "Asleep_Time", "Awake_Time"]].drop_duplicates("GlobalDay").rename(columns={"GlobalDay":"Day_of_Sleep"})

overall_summary["Days_Missing_Sleep"] = sleep_summary_df["Asleep_Time"].isnull().sum()

overall_summary["Prop_Missing_Sleep"] = round(((sleep_summary_df["Asleep_Time"].isnull().sum())/len(sleep_summary_df)),2)


csv_name = "Sleep_Summary_"+RID+".csv"

sleep_summary_df.to_csv(os.path.join(PathKeeper.summarized_data_path,csv_name), index=False)

```

```{python}

#| eval: True
#| echo: false
#| warning: false
#| message: false


prettify_info(HR_summary_df)

```




```{python}

#| eval: True
#| echo: false
#| warning: false
#| message: false


new_presence_row=pd.DataFrame([{
    "ID": RID,
    "HR_Present": has_hr,
    "Sleep_Present": has_sleep,
    "Double_Support_Present": has_double_support,
    "Asymmetry_Present": has_asymmetry, 
    "Cognitive_Survey_Present": has_cognitive_survey,
    "Fatigue_Survey_Present": has_fatique_survey,
    "Sleep_Survey_Present": has_sleep_survey
}])


presence_df = pd.concat([presence_df, new_presence_row], ignore_index=True)

presence_df.drop_duplicates(subset=["ID"], keep="last").to_csv(os.path.join(PathKeeper.data_log_path,"Data_Presence_Log.csv"), index=False)

overall_summary_df=pd.DataFrame([overall_summary])
csv_name = "Overall_Summary_"+RID+".csv"
overall_summary_df.to_csv(os.path.join(PathKeeper.summarized_data_path,csv_name), index=False)
```

### Notices and Metainfo {.unnumbered .unlisted} 

#### Copyright {.unnumbered .unlisted} 

<details>
<notice>

Copyright © 2025 The Regents of the University of Michigan



This file is part of Yawnalyzer.



This program is free software: you can redistribute it and/or modify

it under the terms of the GNU General Public License as published by

the Free Software Foundation, either version 3 of the License, or (at your option) any later version.



This program is distributed in the hope that it will be useful,

but WITHOUT ANY WARRANTY; without even the implied warranty of

MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the

GNU General Public License for more details.



You should have received a copy of the GNU General Public License along

with this program. If not, see <https://www.gnu.org/licenses/>.
</notice>
</details>

#### Session Info{.unnumbered .unlisted} 

```{python}

#| label: Session_Information
#| echo: false
#| warning: false
#| message: false

session_info.show()

```

